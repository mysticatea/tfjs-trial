(window.webpackJsonp=window.webpackJsonp||[]).push([[4],{315:function(e,r,t){"use strict";t.r(r);var o=t(0),n=Object(o.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var e=this,r=e.$createElement,t=e._self._c||r;return t("div",{staticClass:"content"},[t("h1",{attrs:{id:"alphazero-clone"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#alphazero-clone","aria-hidden":"true"}},[e._v("#")]),e._v(" AlphaZero Clone")]),t("p",[e._v("これは、ニューラルネットワークと "),t("a",{attrs:{href:"https://js.tensorflow.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("TensorFlow.js")]),e._v(" に関する理解を深めることを目的として作成した習作です。\n"),t("a",{attrs:{href:"https://js.tensorflow.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("TensorFlow.js")]),e._v(" を利用して実装された AlphaZero クローンになります。")]),t("blockquote",[t("p",[e._v("※ まだちゃんと動きません。")])]),t("ul",[t("li",[t("a",{attrs:{href:"https://js.tensorflow.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("TensorFlow.js")]),e._v(" とは、Google 主導で開発されている機械学習フレームワークである "),t("a",{attrs:{href:"https://www.tensorflow.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("TensorFlow")]),e._v(" を JavaScript (TypeScript) で再実装した公式フレームワークです。"),t("a",{attrs:{href:"https://www.khronos.org/webgl/",target:"_blank",rel:"noopener noreferrer"}},[e._v("WebGL")]),e._v(" を利用して GPU による計算を行えます。Web ブラウザ上で動作するため PC, スマホ, 携帯ゲーム機を含むさまざまな端末で動作し、端末上で深層学習を用いた評価・学習を実施できます。")]),t("li",[e._v("AlphaZero とは、Google 傘下の DeepMind から発表された論文 "),t("a",{attrs:{href:"https://arxiv.org/abs/1712.01815",target:"_blank",rel:"noopener noreferrer"}},[e._v("Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm")]),e._v(" で発表された汎用的な手順最適化アルゴリズムです。深層学習 (Value/Policy Networks) と"),t("a",{attrs:{href:"https://en.wikipedia.org/wiki/Monte_Carlo_tree_search",target:"_blank",rel:"noopener noreferrer"}},[e._v("モンテカルロ木探索")]),e._v("をベースにしたものになっています。")])])])}],!1,null,null,null);r.default=n.exports}}]);